{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessor.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swlee23/Deep-Learning-Time-Series-Anomaly-Detection/blob/master/data_preprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoPHJZYzmxH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asxkJtTfoK4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
        "\n",
        "    def __init__(self, filename, split, cols):\n",
        "        dataframe = pd.read_csv(filename)\n",
        "        i_split = int(len(dataframe) * split)\n",
        "        self.data_train = dataframe.get(cols).values[:i_split]\n",
        "        self.data_test  = dataframe.get(cols).values[i_split:]\n",
        "        self.len_train  = len(self.data_train)\n",
        "        self.len_test   = len(self.data_test)\n",
        "        self.len_train_windows = None\n",
        "\n",
        "    def get_test_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y test data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise reduce size of the training split.\n",
        "        '''\n",
        "        data_windows = []\n",
        "        for i in range(self.len_test - seq_len):\n",
        "            data_windows.append(self.data_test[i:i+seq_len])\n",
        "\n",
        "        data_windows = np.array(data_windows).astype(float)\n",
        "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
        "\n",
        "        x = data_windows[:, :-1]\n",
        "        y = data_windows[:, -1, [0]]\n",
        "        return x,y\n",
        "\n",
        "    def get_train_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y train data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise use generate_training_window() method.\n",
        "        '''\n",
        "        data_x = []\n",
        "        data_y = []\n",
        "        for i in range(self.len_train - seq_len):\n",
        "            x, y = self._next_window(i, seq_len, normalise)\n",
        "            data_x.append(x)\n",
        "            data_y.append(y)\n",
        "        return np.array(data_x), np.array(data_y)\n",
        "\n",
        "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
        "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
        "        i = 0\n",
        "        while i < (self.len_train - seq_len):\n",
        "            x_batch = []\n",
        "            y_batch = []\n",
        "            for b in range(batch_size):\n",
        "                if i >= (self.len_train - seq_len):\n",
        "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
        "                    yield np.array(x_batch), np.array(y_batch)\n",
        "                    i = 0\n",
        "                x, y = self._next_window(i, seq_len, normalise)\n",
        "                x_batch.append(x)\n",
        "                y_batch.append(y)\n",
        "                i += 1\n",
        "            yield np.array(x_batch), np.array(y_batch)\n",
        "\n",
        "    def _next_window(self, i, seq_len, normalise):\n",
        "        '''Generates the next data window from the given index location i'''\n",
        "        window = self.data_train[i:i+seq_len]\n",
        "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
        "        x = window[:-1]\n",
        "        y = window[-1, [0]]\n",
        "        return x, y\n",
        "\n",
        "    def normalise_windows(self, window_data, single_window=False):\n",
        "        '''Normalise window with a base value of zero'''\n",
        "        normalised_data = []\n",
        "        window_data = [window_data] if single_window else window_data\n",
        "        for window in window_data:\n",
        "            normalised_window = []\n",
        "            for col_i in range(window.shape[1]):\n",
        "                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
        "                normalised_window.append(normalised_col)\n",
        "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
        "            normalised_data.append(normalised_window)\n",
        "        return np.array(normalised_data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}